{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-03 09:19:49.176806: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-03 09:19:49.179345: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-03 09:19:49.507364: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-03 09:19:52.657928: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import json\n",
    "import pandas as pd\n",
    "import string\n",
    "import spacy\n",
    "import re\n",
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. convert ether output to input for tf_rel model's input in order\n",
    "2. apply fine-tuned tf_rel model in order\n",
    "3. apply rel_label model in order\n",
    "4. fill the results in dev set\n",
    "5. get res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. convert ether output to input for tf_rel model's input in order\n",
    "cancer = \"breast\"\n",
    "mode = \"dev\"\n",
    "\n",
    "# 1.1 get ether list from timeset/my_code_result/notebooks1/generate_tf_training_input.ipynb\n",
    "# gold_dct_file = f\"/users/the/NER_MTB/0_{cancer}_{mode}_gold_dct.json\"\n",
    "note_dir = os.path.join(\"/users/the/NER_MTB/timelines/chemoTimelinesBaselineSystem/input/change_n2space\", \\\n",
    "                        f\"input_{cancer}_{mode}\", \"Patient_Notes\", f\"{cancer}\", f\"{mode}\")\n",
    "unsummarized_file = f\"/users/the/NER_MTB/timelines/chemoTimelinesBaselineSystem/output/change_n2space/unsummarized_{cancer}_{mode}_output.tsv\"\n",
    "df = pd.read_csv(unsummarized_file, delimiter=\"\\t\")\n",
    "\n",
    "def find_loc(a, b):\n",
    "    a_clean = ''.join(a.split())\n",
    "    b_clean = ''.join(b.split())\n",
    "    index = a_clean.find(b_clean)\n",
    "    s, out = 0, []\n",
    "    for i, char in enumerate(a):\n",
    "        if char == \" \" or char == \"\\t\":\n",
    "            continue \n",
    "        s+=1 \n",
    "        if s == index + 1: \n",
    "            out.append(i)\n",
    "            \n",
    "        if s == index + len(b_clean):\n",
    "            out.append(i)\n",
    "    if len(out) == 2:\n",
    "        end, start = out[0], out[1]+1\n",
    "        return end, start\n",
    "    else: \n",
    "        return None, None\n",
    "\n",
    "\n",
    "def _find_surrounding_text(string):\n",
    "    tlink_inst = string\n",
    "    e_pattern = r'<e>\\s*(.*?)\\s*</e>'\n",
    "    matches = re.findall(e_pattern, tlink_inst)\n",
    "    return matches\n",
    "\n",
    "def _find_time_surrounding_text(string):\n",
    "    tlink_inst = string\n",
    "    t_pattern = r'<t>\\s*(.*?)\\s*</t>'\n",
    "    matches = re.findall(t_pattern, tlink_inst)\n",
    "    return matches\n",
    "\n",
    "def match_ignore_spaces(a, b):\n",
    "    # Create a pattern for string a with optional spaces between each character\n",
    "    pattern_a = r'\\b{}\\b'.format(r'\\s*'.join(re.escape(word) for word in a.split()))\n",
    "    # Find the matched pattern in string b\n",
    "    match = re.search(pattern_a, b)\n",
    "    if match:\n",
    "        return match.start(), match.end()\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "\n",
    "def _find_time_word_index(original_text, given_string, word):\n",
    "    original_text_processed = original_text.translate(str.maketrans(string.punctuation, ' ' * len(string.punctuation)))\n",
    "    word = word.translate(str.maketrans(string.punctuation, ' ' * len(string.punctuation)))\n",
    "\n",
    "    match_s, match_e = match_ignore_spaces(word, original_text_processed)\n",
    "    return match_s, match_e\n",
    "\n",
    "    \n",
    "def same_characters_in_order(s1, s2):\n",
    "    # Remove punctuation from both strings\n",
    "    s1_cleaned = ''.join(char for char in s1 if char not in string.punctuation)\n",
    "    s2_cleaned = ''.join(char for char in s2 if char not in string.punctuation)\n",
    "    # Compare the cleaned strings\n",
    "    return s1_cleaned, s2_cleaned\n",
    "\n",
    "def decrease_space_before_punctuation(text):\n",
    "    # Define the regular expression pattern\n",
    "    pattern = r'(?<=\\w)\\s+(?=\\W)'\n",
    "    # Replace multiple spaces with a single space before punctuation\n",
    "    cleaned_text = re.sub(pattern, ' ', text)\n",
    "    return cleaned_text\n",
    "\n",
    "def _find_tsv_sentences_in_range(doc, start_index, end_index):\n",
    "    \"Given nth sents of chemo and timex, return the context\"\n",
    "    sentences_in_range = []\n",
    "    for ind, sent in enumerate(doc.sents):\n",
    "        if ind >= start_index and ind < end_index:\n",
    "            sentences_in_range.append(sent)\n",
    "    return sentences_in_range\n",
    "\n",
    "def _df_find_nth(lst, match_start_lst, match_end_lst):\n",
    "    \"Used the loc from raw to find where sent it is in Doc\"\n",
    "    nth_sent_lst = []\n",
    "    ith_start_lst, ith_end_lst = [], []\n",
    "    if len(lst)!=0:\n",
    "        for i, chemo in enumerate(lst):\n",
    "            item_start_ind, item_end_ind = match_start_lst[i], match_end_lst[i]\n",
    "            if item_end_ind is None or item_start_ind is None:\n",
    "                print(\"skip here: \", item_start_ind, item_end_ind )\n",
    "                item_end_ind, item_start_ind = -1, -1\n",
    "                # continue\n",
    "\n",
    "            len_ith = item_end_ind - item_start_ind\n",
    "\n",
    "            for sent_id, sent in enumerate(doc.sents):\n",
    "                if sent.start_char <= item_start_ind< sent.end_char: \n",
    "                    ith_start_ind = int(item_start_ind - sent.start_char)\n",
    "                    ith_end_ind = item_start_ind + len_ith\n",
    "                    for token in sent:\n",
    "                        if token.idx <= item_start_ind< (token.idx+len(token.text)):\n",
    "                            nth_sent_lst.append(int(sent_id))\n",
    "                            ith_start_lst.append(ith_start_ind)\n",
    "                            ith_end_lst.append(ith_end_ind)\n",
    "    print(\"lst\", lst)\n",
    "    print(\"nth_sent_lst\", nth_sent_lst)\n",
    "    diff = len(lst) - len(nth_sent_lst)\n",
    "    if len(lst) > len(nth_sent_lst):\n",
    "        while diff > 0:\n",
    "            nth_sent_lst.append(-1)\n",
    "            diff -= 1\n",
    "    assert len(lst) == len(nth_sent_lst)\n",
    "    return nth_sent_lst, ith_start_lst, ith_end_lst\n",
    "    \n",
    "# with open(gold_dct_file, \"r\") as infile: \n",
    "#     gold_dct = json.load(infile)\n",
    "\n",
    "spacy_better = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "pos_rel = []\n",
    "final_df = pd.DataFrame()\n",
    "cnt = 0\n",
    "# for pat_id in [\"patient47\"]:\n",
    "# for pat_id in gold_dct.keys():\n",
    "#     note_sub_dir = os.path.join(note_dir, pat_id)\n",
    "#     medium_df = pd.DataFrame()\n",
    "#     for filename in gold_dct[pat_id].keys():\n",
    "for pat_id in os.listdir(note_dir):\n",
    "    if os.path.isdir(os.path.join(note_dir, pat_id)):\n",
    "        print(pat_id)\n",
    "        for pat_filename in os.listdir(os.path.join(note_dir, pat_id)):\n",
    "            filename = \"_\".join(pat_filename.split(\".txt\")[0].split(\"_\")[1:])\n",
    "            print(filename)\n",
    "            raw_note_file = os.path.join(note_dir, pat_id, pat_filename)\n",
    "            with open(raw_note_file, \"r\") as infile: \n",
    "                lines = infile.readlines()\n",
    "            num_pre9_tokens = len(''.join(lines[:9]))\n",
    "            lines = lines[9:]\n",
    "            clean_note = \"\".join(lines)\n",
    "            doc = spacy_better(clean_note)\n",
    "\n",
    "            chemo_start_ind_tlinkinst, chemo_end_ind_tlinkinst = [], []\n",
    "            nth_sent = []\n",
    "            chemo_lst = []\n",
    "            time_start_ind_tlinkinst, time_end_ind_tlinkinst = [], []\n",
    "            time_nth_sent = []\n",
    "            raw_time_lst = []\n",
    "            match_sents = []\n",
    "\n",
    "            selected_i = []\n",
    "            print(df.loc[df[\"note_name\"]==\"patient01_report011_RAD\"][\"tlink_inst\"].iloc[0])\n",
    "            print(df.loc[df[\"note_name\"]==\"patient01_report011_RAD\"][\"tlink_inst\"].iloc[1])\n",
    "            print(df.loc[df[\"note_name\"]==\"patient01_report011_RAD\"][\"tlink_inst\"].iloc[2])\n",
    "            patient_file = df.loc[df[\"note_name\"] == pat_id + \"_\" + filename]\n",
    "            tsv_df = pd.DataFrame()\n",
    "            selected_row = []\n",
    "            for i, row in patient_file.iterrows():\n",
    "                tlink = \"none\"\n",
    "\n",
    "                if row[\"chemo_text\"] != \"none\" and row[\"normed_timex\"] != \"none\":\n",
    "                    tlink_inst = row[\"tlink_inst\"]\n",
    "                    matches = _find_surrounding_text(tlink_inst)\n",
    "                    time_matches = _find_time_surrounding_text(tlink_inst)\n",
    "                    if pat_id == \"patient01\" and filename==\"report011_RAD\":\n",
    "                        print(time_matches)\n",
    "\n",
    "                    selected_row.append(row)\n",
    "                    \n",
    "                    if len(matches) == 0 or len(time_matches) == 0:\n",
    "                        if len(matches) == 0:\n",
    "                            print(\"chemo: yese\", matches)\n",
    "                        elif len(time_matches) == 0:\n",
    "                            print(\"1time: \")\n",
    "\n",
    "                    clean_match = matches[0].translate(str.maketrans(string.punctuation, ' ' * len(string.punctuation)))\n",
    "                    time_clean_match = time_matches[0].translate(str.maketrans(string.punctuation, ' ' * len(string.punctuation)))\n",
    "                    \n",
    "                    chemo_lst.append(clean_match)\n",
    "                    raw_time_lst.append(time_clean_match) # not exact raw time, but if use the index it is fine\n",
    "                    \n",
    "                    start_ind, end_ind = find_loc(clean_note, clean_match)\n",
    "                    time_start_ind, time_end_ind = _find_time_word_index(clean_note, tlink_inst, time_clean_match)\n",
    "\n",
    "                    chemo_start_ind_tlinkinst.append(start_ind)\n",
    "                    chemo_end_ind_tlinkinst.append(end_ind)\n",
    "                    time_start_ind_tlinkinst.append(time_start_ind)\n",
    "                    time_end_ind_tlinkinst.append(time_end_ind)\n",
    "    \n",
    "            nth_sent_item, ith_start_item, ith_end_item = _df_find_nth(chemo_lst, chemo_start_ind_tlinkinst, chemo_end_ind_tlinkinst) \n",
    "            time_nth_sent_item, time_ith_start_item, time_ith_end_item = _df_find_nth(raw_time_lst, time_start_ind_tlinkinst, time_end_ind_tlinkinst)\n",
    "\n",
    "            tsv_df[\"chemo_tlinkinst_start\"] = chemo_start_ind_tlinkinst\n",
    "            tsv_df[\"chemo_tlinkinst_end\"] = chemo_end_ind_tlinkinst\n",
    "            tsv_df[\"nth_sent\"] = nth_sent_item\n",
    "\n",
    "            tsv_df[\"time_tlinkinst_start\"] = time_start_ind_tlinkinst\n",
    "            tsv_df[\"time_tlinkinst_end\"] = time_end_ind_tlinkinst\n",
    "            tsv_df[\"time_nth_sent\"] = time_nth_sent_item\n",
    "            tsv_df[\"time_ith_start\"] = time_ith_start_item\n",
    "            tsv_df[\"time_ith_end\"] = time_ith_end_item\n",
    "            tsv_df[\"raw_time\"] = raw_time_lst\n",
    "\n",
    "            sent_range_lst = []\n",
    "            for i in range(len(time_nth_sent_item)):\n",
    "                if int(nth_sent_item[i]) > int(time_nth_sent_item[i]):\n",
    "                    start_nth_sent = int(time_nth_sent_item[i])\n",
    "                    end_nth_sent = int(nth_sent_item[i])\n",
    "                else: \n",
    "                    start_nth_sent = int(nth_sent_item[i])\n",
    "                    end_nth_sent = int(time_nth_sent_item[i])\n",
    "                sent_range = _find_tsv_sentences_in_range(doc, start_nth_sent, end_nth_sent+1)\n",
    "                sent_range_lst.append(sent_range) \n",
    "            tsv_df[\"match_sents\"] = sent_range_lst\n",
    "            selected_pat_file = patient_file.iloc[selected_i]\n",
    "            selected_pat_file = pd.DataFrame(columns=[\"DCT\", \"patient_id\", \"chemo_text\", \"chemo_annotation_id\", \"normed_timex\", \"timex_annotation_id\", \"tlink\", \"note_name\", \"tlink_inst\"], data=selected_row)\n",
    "            new_df = pd.concat([selected_pat_file.reset_index(), tsv_df.reset_index()], axis=1)\n",
    "            medium_df = pd.concat([new_df, medium_df])\n",
    "    final_df = pd.concat([final_df, medium_df])\n",
    "final_df = final_df.reset_index()\n",
    "\n",
    "# final_df.to_csv(f\"./inference/{cancer}_{mode}_wo_labels.tsv\", sep=\"\\t\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_df[final_df[\"note_name\"]==\"patient01_report011_RAD\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "note_dir = os.path.join(\"/users/the/NER_MTB/timelines/chemoTimelinesBaselineSystem/input/change_n2space\", \\\n",
    "                        f\"input_{cancer}_{mode}\", \"Patient_Notes\", f\"{cancer}\", f\"{mode}\")\n",
    "tsv_lst = []\n",
    "pat_filename = np.unique(final_df[\"note_name\"]).tolist()\n",
    "for uni_pat_filename in pat_filename:\n",
    "    single_dct = defaultdict(dict)\n",
    "    single_df = final_df[final_df[\"note_name\"] == uni_pat_filename]\n",
    "    # print(single_df.columns)\n",
    "    print(single_df)\n",
    "    for i, row in single_df.iterrows():\n",
    "        print(i, row)\n",
    "        if abs(row[\"nth_sent\"]-row[\"time_nth_sent\"]) <= 1:\n",
    "            single_dct[\"filename\"] = uni_pat_filename\n",
    "            single_dct[\"statement\"] = \"Does the arg1 and arg2 has relationship?\"\n",
    "            single_dct[\"arg1\"] = {\n",
    "                \"mention\": row[\"chemo_text\"],\n",
    "                \"nth_sent\": int(row[\"nth_sent\"])\n",
    "            }\n",
    "            single_dct[\"arg2\"] = {\n",
    "                \"mention\": row[\"raw_time\"],\n",
    "                \"time_tlinkinst_start\": int(row[\"time_tlinkinst_start\"]),\n",
    "                \"time_tlinkinst_end\": int(row[\"time_tlinkinst_end\"]),\n",
    "                \"time_nth_sent\": int(row[\"time_nth_sent\"])\n",
    "            }\n",
    "            \n",
    "            pat_id = uni_pat_filename.split(\"_\")[0]\n",
    "            note_sub_dir = os.path.join(note_dir, pat_id)\n",
    "            pat_filename_txt = uni_pat_filename+\".txt\"\n",
    "            raw_note_file = os.path.join(note_sub_dir, pat_filename_txt)\n",
    "\n",
    "            with open(raw_note_file, \"r\") as infile: \n",
    "                lines = infile.readlines()\n",
    "            num_pre9_tokens = len(''.join(lines[:9]))\n",
    "            single_dct[\"num_pre9_tokens\"] = num_pre9_tokens \n",
    "\n",
    "            lines = lines[9:]\n",
    "            clean_note = \"\".join(lines)\n",
    "            doc = spacy_better(clean_note)\n",
    "            \n",
    "            if int(row[\"nth_sent\"]) < int(row[\"time_nth_sent\"]):\n",
    "                start_sent_ind = int(row[\"nth_sent\"])\n",
    "                end_start_ind = int(row[\"time_nth_sent\"])\n",
    "            else: \n",
    "                start_sent_ind = int(row[\"time_nth_sent\"])\n",
    "                end_start_ind = int(row[\"nth_sent\"])\n",
    "            \n",
    "            temp = []\n",
    "            for i, sent in enumerate(doc.sents):\n",
    "                if i<= end_start_ind and i >= start_sent_ind:\n",
    "                    temp.append(sent.text)\n",
    "            single_dct[\"context\"] = \" \".join(temp)\n",
    "            single_dct[\"tlink\"] = \"BEGINS-ON\" #\"none\"\n",
    "            single_dct[\"rel_type\"] = \"positive\"\n",
    "            print(single_dct)\n",
    "            tsv_lst.append(single_dct)\n",
    "\n",
    "def _decrease_space_before_punctuation(text):\n",
    "    # Define the regular expression pattern\n",
    "    pattern = r'(?<=\\w)\\s+(?=\\W)'\n",
    "    # Replace multiple spaces with a single space before punctuation\n",
    "    cleaned_text = re.sub(pattern, ' ', text)\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "# def _further_clean_json(dir, cancer, mode):\n",
    "def _further_clean_json(tsv_lst):\n",
    "    \n",
    "    final_neg_lst = []\n",
    "    \n",
    "    #infile = f\"/users/the/NER_MTB/timeset/my_code_result/input_data/tf_rel_task/inference/{cancer}/{mode}/test.json\"\n",
    "    # infile = os.path.join(dir, cancer, mode, \"test.json\")\n",
    "    # with open(infile, \"r\") as file: \n",
    "    #     input_lst = json.load(file)\n",
    "\n",
    "    for neg in tsv_lst:\n",
    "        context = neg[\"context\"]\n",
    "        ment1 = neg[\"arg1\"][\"mention\"]\n",
    "        ment2 =neg[\"arg2\"][\"mention\"]\n",
    "        cleaned_context = _decrease_space_before_punctuation(context)\n",
    "        cleaned_ment1, cleaned_ment2 = _decrease_space_before_punctuation(ment1), _decrease_space_before_punctuation(ment2)\n",
    "        final_neg_lst.append({\n",
    "            \"filename\" :neg[\"filename\"],\n",
    "            \"statement\" :neg[\"statement\"],\n",
    "            \"label\":neg[\"label\"],\n",
    "            \"relation\":neg[\"relation\"],\n",
    "            # \"num_pre9_tokens\":neg[\"num_pre9_tokens\"],\n",
    "            # \"rel_type\":\"BEGINS-ON\",\n",
    "            # \"label\":\"positive\",\n",
    "            \"arg1\":{\n",
    "                \"mention\":cleaned_ment1,\n",
    "                # \"nth_sent\":neg[\"arg1\"][\"nth_sent\"],\n",
    "                \"start\":find_loc(cleaned_context,cleaned_ment1)[0],\n",
    "                \"end\":find_loc(cleaned_context,cleaned_ment1)[1]\n",
    "            },\n",
    "            \"arg2\":{\n",
    "                \"mention\":cleaned_ment2,\n",
    "                # \"nth_sent\":neg[\"arg2\"][\"nth_sent\"],\n",
    "                \"start\":find_loc(cleaned_context,cleaned_ment2)[0],\n",
    "                \"end\":find_loc(cleaned_context,cleaned_ment2)[1]\n",
    "            }, \"context\":cleaned_context\n",
    "\n",
    "        })\n",
    "\n",
    "final_lst = _further_clean_json(tsv_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsv_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_loc(a, b):\n",
    "    # a: long one\n",
    "    a = re.sub('[^0-9a-zA-Z]', ' ', a)\n",
    "    b = re.sub('[^0-9a-zA-Z]', ' ', b)\n",
    "    a_clean = ''.join(a.split())\n",
    "    b_clean = ''.join(b.split())\n",
    "    index = a_clean.find(b_clean)\n",
    "    s, out = 0, []\n",
    "    for i, char in enumerate(a):\n",
    "        if char == \" \" or char == \"\\t\":\n",
    "            continue \n",
    "        s+=1 \n",
    "        if s == index + 1: \n",
    "            out.append(i)           \n",
    "        if s == index + len(b_clean):\n",
    "            out.append(i)\n",
    "    if len(out) == 2:\n",
    "        start, end = out[0], out[1]+1\n",
    "        return start, end\n",
    "    else: \n",
    "        return None, None\n",
    "\n",
    "final_neg_lst = []\n",
    "\n",
    "for neg in tsv_lst:\n",
    "    context = neg[\"context\"]\n",
    "\n",
    "    ment1 = neg[\"arg1\"][\"mention\"]\n",
    "    ment2 =neg[\"arg2\"][\"mention\"]\n",
    "    final_neg_lst.append({\n",
    "        \"filename\" :neg[\"filename\"],\n",
    "        \"statement\" :neg[\"statement\"],\n",
    "        \"num_pre9_tokens\":neg[\"num_pre9_tokens\"],\n",
    "        \"rel_type\":\"BEGINS-ON\",\n",
    "        \"label\":\"positive\",\n",
    "        \"arg1\":{\n",
    "            \"mention\":neg[\"arg1\"][\"mention\"],\n",
    "            \"nth_sent\":neg[\"arg1\"][\"nth_sent\"],\n",
    "            \"start\":find_loc(context,ment1)[0],\n",
    "            \"end\":find_loc(context,ment1)[1]\n",
    "        },\n",
    "        \"arg2\":{\n",
    "            \"mention\":neg[\"arg2\"][\"mention\"],\n",
    "            \"nth_sent\":neg[\"arg2\"][\"time_nth_sent\"],\n",
    "            \"start\":find_loc(context,ment2)[0],\n",
    "            \"end\":find_loc(context,ment2)[1]\n",
    "        }, \"context\":context\n",
    "\n",
    "    })\n",
    "\n",
    "    final_neg_lst\n",
    "\n",
    "# with open(f\"/users/the/NER_MTB/timeset/my_code_result/model/tf_rel_task/temporal-nli/inference/{cancer}_{mode}_train_tf_input.json\", \"w\") as outfile: \n",
    "with open(f\"/users/the/NER_MTB/timeset/my_code_result/input_data/tf_rel_task/inference/{cancer}/{mode}/test.json\", \"w\") as outfile: \n",
    "    json.dump(final_neg_lst, outfile, indent=4)\n",
    "# test.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_neg_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. apply fine-tuned tf_rel model in order\n",
    "\n",
    "# /users/the/NER_MTB/timeset/my_code_result/scripts/benchmark/tf_rel_task/inference_ft_deberta-v3-large_bfloat16.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to change the code later\n",
    "\n",
    "tf_rel_lst = []\n",
    "\n",
    "for rel in final_neg_lst:\n",
    "    context = rel[\"context\"]\n",
    "    ment1 = rel[\"arg1\"][\"mention\"]\n",
    "    ment2 =rel[\"arg2\"][\"mention\"]\n",
    "    tf_rel_lst.append({\n",
    "        \"filename\" :rel[\"filename\"],\n",
    "        \"statement\" :rel[\"statement\"],\n",
    "        \"num_pre9_tokens\":rel[\"num_pre9_tokens\"],\n",
    "        \"rel_type\":\"BEGINS-ON\",\n",
    "        \"label\":\"positive\",\n",
    "        \"arg1\":{\n",
    "            \"mention\":rel[\"arg1\"][\"mention\"],\n",
    "            \"nth_sent\":rel[\"arg1\"][\"nth_sent\"],\n",
    "            \"start\":rel[\"arg1\"][\"start\"],\n",
    "            \"end\":rel[\"arg1\"][\"end\"]\n",
    "        },\n",
    "        \"arg2\":{\n",
    "            \"mention\":rel[\"arg2\"][\"mention\"],\n",
    "            \"nth_sent\":rel[\"arg2\"][\"nth_sent\"],\n",
    "            \"start\":rel[\"arg2\"][\"start\"],\n",
    "            \"end\":rel[\"arg2\"][\"end\"]\n",
    "        }, \"context\":rel[\"context\"]\n",
    "\n",
    "    })\n",
    "\n",
    "    tf_rel_lst\n",
    "\n",
    "\n",
    "temp_file = f\"/users/the/NER_MTB/timeset/my_code_result/input_data/tf_rel_task/inference/{cancer}/{mode}\"\n",
    "os.makedirs(temp_file, exist_ok=True)\n",
    "\n",
    "with open(os.path.join(f\"{temp_file}/test.json\"), \"w\") as outfile: \n",
    "    json.dump(tf_rel_lst, outfile, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# further clean dataset by removing the space \n",
    "cancer = \"breast\"\n",
    "mode = \"dev\"\n",
    "\n",
    "def find_loc(a, b):\n",
    "    # a: long one\n",
    "    a = re.sub('[^0-9a-zA-Z]', ' ', a)\n",
    "    b = re.sub('[^0-9a-zA-Z]', ' ', b)\n",
    "    a_clean = ''.join(a.split())\n",
    "    b_clean = ''.join(b.split())\n",
    "    index = a_clean.find(b_clean)\n",
    "    s, out = 0, []\n",
    "    for i, char in enumerate(a):\n",
    "        if char == \" \" or char == \"\\t\":\n",
    "            continue \n",
    "        s+=1 \n",
    "        if s == index + 1: \n",
    "            out.append(i)           \n",
    "        if s == index + len(b_clean):\n",
    "            out.append(i)\n",
    "    if len(out) == 2:\n",
    "        start, end = out[0], out[1]+1\n",
    "        return start, end\n",
    "    else: \n",
    "        return None, None\n",
    "\n",
    "def _decrease_space_before_punctuation(text):\n",
    "    # Define the regular expression pattern\n",
    "    pattern = r'(?<=\\w)\\s+(?=\\W)'\n",
    "    # Replace multiple spaces with a single space before punctuation\n",
    "    cleaned_text = re.sub(pattern, ' ', text)\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "def _further_clean_json(dir, cancer, mode):\n",
    "    \n",
    "    final_neg_lst = []\n",
    "    \n",
    "    #infile = f\"/users/the/NER_MTB/timeset/my_code_result/input_data/tf_rel_task/inference/{cancer}/{mode}/test.json\"\n",
    "    infile = os.path.join(dir, cancer, mode, \"test.json\")\n",
    "    with open(infile, \"r\") as file: \n",
    "        input_lst = json.load(file)\n",
    "\n",
    "    for neg in input_lst:\n",
    "        context = neg[\"context\"]\n",
    "        ment1 = neg[\"arg1\"][\"mention\"]\n",
    "        ment2 =neg[\"arg2\"][\"mention\"]\n",
    "        cleaned_context = _decrease_space_before_punctuation(context)\n",
    "        cleaned_ment1, cleaned_ment2 = _decrease_space_before_punctuation(ment1), _decrease_space_before_punctuation(ment2)\n",
    "        final_neg_lst.append({\n",
    "            \"filename\" :neg[\"filename\"],\n",
    "            \"statement\" :neg[\"statement\"],\n",
    "            \"label\":neg[\"label\"],\n",
    "            \"relation\":neg[\"relation\"],\n",
    "            # \"num_pre9_tokens\":neg[\"num_pre9_tokens\"],\n",
    "            # \"rel_type\":\"BEGINS-ON\",\n",
    "            # \"label\":\"positive\",\n",
    "            \"arg1\":{\n",
    "                \"mention\":cleaned_ment1,\n",
    "                # \"nth_sent\":neg[\"arg1\"][\"nth_sent\"],\n",
    "                \"start\":find_loc(cleaned_context,cleaned_ment1)[0],\n",
    "                \"end\":find_loc(cleaned_context,cleaned_ment1)[1]\n",
    "            },\n",
    "            \"arg2\":{\n",
    "                \"mention\":cleaned_ment2,\n",
    "                # \"nth_sent\":neg[\"arg2\"][\"nth_sent\"],\n",
    "                \"start\":find_loc(cleaned_context,cleaned_ment2)[0],\n",
    "                \"end\":find_loc(cleaned_context,cleaned_ment2)[1]\n",
    "            }, \"context\":cleaned_context\n",
    "\n",
    "        })\n",
    "\n",
    "        final_neg_lst\n",
    "    # with open(f\"/users/the/NER_MTB/timeset/my_code_result/input_data/tf_rel_task/inference/{cancer}/{mode}/test.json\", \"w\") as outfile:\n",
    "    with open(infile, \"w\") as outfile: \n",
    "        json.dump(final_neg_lst, outfile, indent=4)\n",
    "\n",
    "dir = \"/users/the/NER_MTB/timeset/my_code_result/input_data/tf_rel_task/inference\"\n",
    "_further_clean_json(dir, cancer, mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. apply rel_label model in order\n",
    "# need to change the code later\n",
    "cancer = \"breast\"\n",
    "mode = \"dev\"\n",
    "\n",
    "with open(f\"/users/the/NER_MTB/timeset/my_code_result/input_data/tf_rel_task/inference/{cancer}/{mode}/test.json\", \"r\") as infile: \n",
    "    final_neg_lst = json.load(infile) \n",
    "\n",
    "tf_rel_lst = []\n",
    "\n",
    "for rel in final_neg_lst:\n",
    "    context = rel[\"context\"]\n",
    "    ment1 = rel[\"arg1\"][\"mention\"]\n",
    "    ment2 =rel[\"arg2\"][\"mention\"]\n",
    "    tf_rel_lst.append({\n",
    "        \"filename\" :rel[\"filename\"],\n",
    "        # \"statement\" :rel[\"statement\"],\n",
    "        \"relation\":\"BEGINS-ON\",\n",
    "        # \"label\":\"positive\",\n",
    "        \"arg1\":{\n",
    "            \"mention\":rel[\"arg1\"][\"mention\"],\n",
    "            \"start\":rel[\"arg1\"][\"start\"],\n",
    "            \"end\":rel[\"arg1\"][\"end\"]\n",
    "        },\n",
    "        \"arg2\":{\n",
    "            \"mention\":rel[\"arg2\"][\"mention\"],\n",
    "            \"start\":rel[\"arg2\"][\"start\"],\n",
    "            \"end\":rel[\"arg2\"][\"end\"]\n",
    "        }, \"context\":rel[\"context\"]\n",
    "\n",
    "    })\n",
    "\n",
    "    tf_rel_lst\n",
    "\n",
    "\n",
    "temp_file = f\"/users/the/NER_MTB/timeset/my_code_result/input_data/rel_label_task/inference/{cancer}/{mode}\"\n",
    "os.makedirs(temp_file, exist_ok=True)\n",
    "\n",
    "with open(os.path.join(f\"{temp_file}/test.json\"), \"w\") as outfile: \n",
    "    json.dump(tf_rel_lst, outfile, indent=4)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
