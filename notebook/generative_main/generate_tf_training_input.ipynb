{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-01 23:09:09.473928: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-01 23:09:10.145767: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-01 23:09:11.417131: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-01 23:09:17.117648: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import json\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import re\n",
    "import string\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "import numpy as np\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Covert the gold training chemo data as temporal_nli format \n",
    "1. from gold pair-wise file to add nth_sent for both chemo and timex mentions\n",
    "2. at baseline pair-wise pred output, add start_ind, end_ind, nth_sent for each mentions\n",
    "3. generate candiatate pair from 2, mark the pair as negative if it is not at 1 (need index to compare)\n",
    "4. for each nth sent has potential rel, generate the dct with args and rel \n",
    "##### 5. repeat for single patient_file as dct (count positive and negative rels  the distribution, if little negative, need to think way to improve)\n",
    "##### 6. repeat for cancer_mode with all patients and files as json file\n",
    "##### 7. repeat for all cancer by combing training and dev json file\n",
    "##### 8. random choose x% patient from 6 and merge as a x%_sample_json file\n",
    "\n",
    "\n",
    "\n",
    "##### 3. save file as json for each cancer train\n",
    "##### 4. save file as json for all cancer train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 1. from gold pair-wise file to add nth_sent for both chemo and timex mentions\n",
    "cancer, mode = \"breast\", \"dev\"\n",
    "gold_dct_file = f\"/users/the/NER_MTB/0_{cancer}_{mode}_gold_dct.json\"\n",
    "# here has been replace /n to space\n",
    "note_dir = os.path.join(\"/users/the/NER_MTB/timelines/chemoTimelinesBaselineSystem/input/change_n2space\", \\\n",
    "                        f\"input_{cancer}_{mode}\", \"Patient_Notes\", f\"{cancer}\", f\"{mode}\")\n",
    "unsummarized_file = f\"/users/the/NER_MTB/timelines/chemoTimelinesBaselineSystem/output/change_n2space/unsummarized_{cancer}_{mode}_output.tsv\"\n",
    "df = pd.read_csv(unsummarized_file, delimiter=\"\\t\")\n",
    "\n",
    "def find_loc(a, b):\n",
    "    # a: long one\n",
    "    a_clean = ''.join(a.split())\n",
    "    b_clean = ''.join(b.split())\n",
    "    index = a_clean.find(b_clean)\n",
    "    s, out = 0, []\n",
    "    for i, char in enumerate(a):\n",
    "        if char == \" \" or char == \"\\t\":\n",
    "            continue \n",
    "        s+=1 \n",
    "        if s == index + 1: \n",
    "            out.append(i)\n",
    "            \n",
    "        if s == index + len(b_clean):\n",
    "            out.append(i)\n",
    "    # if len(out) == 1: \n",
    "    #     end, start = None, None\n",
    "\n",
    "    # elif len(out) == 2:  # else\n",
    "    if len(out) == 2:\n",
    "        end, start = out[0], out[1]+1\n",
    "        return end, start\n",
    "    else: \n",
    "        return None, None\n",
    "\n",
    "def _get_seindexs_single_note(string, spacy_better):\n",
    "    \"\"\"Return the inedx of sent and sent_lst after doc\"\"\"\n",
    "    doc = spacy_better(string)\n",
    "    seindexs_single_note = [] \n",
    "    sent_lst = []\n",
    "    for i, sent in enumerate(doc.sents):\n",
    "        seindexs_single_note.append((i, sent.start_char, sent.end_char))\n",
    "        sent_lst.append(sent.text)\n",
    "    return seindexs_single_note, sent_lst\n",
    "\n",
    "def _find_nthind(tuple_list, item_start_ind, num_pre9_tokens):\n",
    "    for tup in tuple_list:\n",
    "        if tup[1] <= (int(item_start_ind)-num_pre9_tokens) <= tup[2]:\n",
    "            return tup[0]\n",
    "    print(\"something wrong with this case\") # TO-DO: need to fix here, run the below code first since little error msgs\n",
    "    return None  # Return None if no match is found # across the sentences\n",
    "\n",
    "def _find_surrounding_text(string):\n",
    "    tlink_inst = string\n",
    "    e_pattern = r'<e>\\s*(.*?)\\s*</e>'\n",
    "    matches = re.findall(e_pattern, tlink_inst)\n",
    "    return matches\n",
    "\n",
    "def _find_time_surrounding_text(string):\n",
    "    tlink_inst = string\n",
    "    t_pattern = r'<t>\\s*(.*?)\\s*</t>'\n",
    "    matches = re.findall(t_pattern, tlink_inst)\n",
    "    return matches\n",
    "\n",
    "def match_ignore_spaces(a, b):\n",
    "    # Create a pattern for string a with optional spaces between each character\n",
    "    pattern_a = r'\\b{}\\b'.format(r'\\s*'.join(re.escape(word) for word in a.split()))\n",
    "    # Find the matched pattern in string b\n",
    "    match = re.search(pattern_a, b)\n",
    "    if match:\n",
    "        return match.start(), match.end()\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "\n",
    "def _find_time_word_index(original_text, given_string, word):\n",
    "    original_text_processed = original_text.translate(str.maketrans(string.punctuation, ' ' * len(string.punctuation)))\n",
    "    # original_text_processed = re.sub(r'\\s+', ' ', original_text_processed)\n",
    "    word = word.translate(str.maketrans(string.punctuation, ' ' * len(string.punctuation)))\n",
    "\n",
    "    match_s, match_e = match_ignore_spaces(word, original_text_processed)\n",
    "    # print(match_s, match_e)\n",
    "    return match_s, match_e\n",
    "\n",
    "\n",
    "def _find_chemo_word_index(original_text, given_string, word):\n",
    "    # print(len(word), word)\n",
    "    # given_string_processed = given_string.translate(str.maketrans(string.punctuation, ' ' * len(string.punctuation)))\n",
    "    original_text_processed = original_text.translate(str.maketrans(string.punctuation, ' ' * len(string.punctuation)))\n",
    "    # original_text_processed = re.sub(r'\\s+', ' ', original_text_processed)\n",
    "    word = word.translate(str.maketrans(string.punctuation, ' ' * len(string.punctuation)))\n",
    "    # print(\"2:\", len(word), word)\n",
    "    # word = re.sub(r'\\s+', ' ', word)\n",
    "    # print(word, \"***\", original_text_processed)\n",
    "    \n",
    "    # edge case\n",
    "    # if word == \"t   c\": \n",
    "    #     word = \"t c\"\n",
    "    #     match = re.search(r'{}'.format(re.escape(word)), original_text_processed)\n",
    "    # elif word == \"T   C\":\n",
    "    #     word = \"T C\"\n",
    "    #     match = re.search(r'{}'.format(re.escape(word)), original_text_processed)\n",
    "    # elif word == \"alpha   2b interferon\":\n",
    "    #     word = \"alpha 2b interferon\"\n",
    "    #     match = re.search(r'{}'.format(re.escape(word)), original_text_processed)\n",
    "    # elif word == \"interleukin   2\": \n",
    "    #     word = \"interleukin 2\"\n",
    "    #     match = re.search(r'{}'.format(re.escape(word)), original_text_processed) \n",
    "    # else: \n",
    "    #     match = re.search(r'\\b{}\\b'.format(re.escape(word)), original_text_processed)\n",
    "    # if match:\n",
    "    #     # print(match.start(), match.end())\n",
    "    #     return match.start(), match.end()\n",
    "    # else:\n",
    "    #     return None, None\n",
    "    \n",
    "def same_characters_in_order(s1, s2):\n",
    "    # Remove punctuation from both strings\n",
    "    s1_cleaned = ''.join(char for char in s1 if char not in string.punctuation)\n",
    "    s2_cleaned = ''.join(char for char in s2 if char not in string.punctuation)\n",
    "    # Compare the cleaned strings\n",
    "    return s1_cleaned, s2_cleaned\n",
    "\n",
    "def decrease_space_before_punctuation(text):\n",
    "    # Define the regular expression pattern\n",
    "    pattern = r'(?<=\\w)\\s+(?=\\W)'\n",
    "    # Replace multiple spaces with a single space before punctuation\n",
    "    cleaned_text = re.sub(pattern, ' ', text)\n",
    "    return cleaned_text\n",
    "\n",
    "def _find_nth(lst, chemo_or_timex):\n",
    "    \"Used the loc from raw to find where sent it is in Doc\"\n",
    "    nth_sent_lst = []\n",
    "    if len(lst)!=0:\n",
    "        for i, chemo in enumerate(lst):\n",
    "            if chemo_or_timex == \"chemo\":\n",
    "                item_start_ind, item_end_ind = gold_dct[pat_id][filename][\"pair_wise\"][\"chemo\"][\"span\"][i][0], gold_dct[pat_id][filename][\"pair_wise\"][\"chemo\"][\"span\"][i][1]\n",
    "            else:\n",
    "                item_start_ind, item_end_ind = gold_dct[pat_id][filename][\"pair_wise\"][\"time\"][\"span\"][i][0], gold_dct[pat_id][filename][\"pair_wise\"][\"time\"][\"span\"][i][1]\n",
    "\n",
    "            # print(item_start_ind, item_end_ind)\n",
    "            for sent_id, sent in enumerate(doc.sents):\n",
    "                # print(sent.start_char, sent.end_char)\n",
    "                if sent.start_char <= item_start_ind-num_pre9_tokens< sent.end_char: \n",
    "                    for token in sent:\n",
    "                        if token.idx <= item_start_ind-num_pre9_tokens< (token.idx+len(token.text)):\n",
    "                            # print(\"Token:\", token)\n",
    "                            # print(\"Sentence:\", sent)\n",
    "                            # print(\"Sentence ID:\", sent_id)\n",
    "                            print(pat_id, filename)\n",
    "                            nth_sent_lst.append(sent_id)\n",
    "                        else: \n",
    "                            print(\"Token:\", token)\n",
    "                            print(\"Sentence:\", sent)\n",
    "                            print(\"Sentence ID:\", sent_id) \n",
    "\n",
    "    print(len(lst), len(nth_sent_lst))\n",
    "    assert len(lst) == len(nth_sent_lst)\n",
    "    return nth_sent_lst\n",
    "\n",
    "def _find_tsv_sentences_in_range(doc, start_index, end_index):\n",
    "    \"Given nth sents of chemo and timex, return the context\"\n",
    "    sentences_in_range = []\n",
    "    for ind, sent in enumerate(doc.sents):\n",
    "        if ind >= start_index and ind < end_index:\n",
    "            sentences_in_range.append(sent)\n",
    "    return sentences_in_range\n",
    "\n",
    "def _find_sentences_in_range(doc, start_index, end_index):\n",
    "    \"Given nth sents of chemo and timex, return the context\"\n",
    "    sentences_in_range = []\n",
    "    for ind, sent in enumerate(doc.sents):\n",
    "        if ind == start_index:\n",
    "            sentences_in_range.append(sent.text)\n",
    "            ind += 1\n",
    "            while ind < end_index:\n",
    "                sentences_in_range.append(list(doc.sents)[ind].text)\n",
    "                ind += 1\n",
    "            return sentences_in_range\n",
    "\n",
    "def _df_find_nth(lst, match_start_lst, match_end_lst):\n",
    "    \"Used the loc from raw to find where sent it is in Doc\"\n",
    "    nth_sent_lst = []\n",
    "    ith_start_lst, ith_end_lst = [], []\n",
    "    if len(lst)!=0:\n",
    "        for i, chemo in enumerate(lst):\n",
    "            item_start_ind, item_end_ind = match_start_lst[i], match_end_lst[i]\n",
    "            if item_end_ind is None or item_start_ind is None:\n",
    "                print(\"skip here: \", item_start_ind, item_end_ind )\n",
    "                item_end_ind, item_start_ind = -1, -1\n",
    "                # continue\n",
    "\n",
    "            len_ith = item_end_ind - item_start_ind\n",
    "\n",
    "            # print(\"chemo: \", item_start_ind, item_end_ind)\n",
    "            for sent_id, sent in enumerate(doc.sents):\n",
    "                # print(sent.start_char, sent.end_char)\n",
    "                if sent.start_char <= item_start_ind< sent.end_char: \n",
    "                    # print(\"yes\")\n",
    "                    ith_start_ind = int(item_start_ind - sent.start_char)\n",
    "                    ith_end_ind = item_start_ind + len_ith\n",
    "                    for token in sent:\n",
    "                        if token.idx <= item_start_ind< (token.idx+len(token.text)):\n",
    "                            # print(\"Token:\", token)\n",
    "                            # print(\"Sentence:\", sent)\n",
    "                            # print(\"Sentence ID:\", sent_id)\n",
    "                            nth_sent_lst.append(int(sent_id))\n",
    "                            ith_start_lst.append(ith_start_ind)\n",
    "                            ith_end_lst.append(ith_end_ind)\n",
    "                            # print(\"\\n\")\n",
    "                # else:\n",
    "                #     nth_sent_lst.append(-1)\n",
    "                #     ith_start_lst.append(-1)\n",
    "                #     ith_end_lst.append(-1)\n",
    "    print(\"lst\", lst)\n",
    "    print(\"nth_sent_lst\", nth_sent_lst)\n",
    "    diff = len(lst) - len(nth_sent_lst)\n",
    "    if len(lst) > len(nth_sent_lst):\n",
    "        while diff > 0:\n",
    "            nth_sent_lst.append(-1)\n",
    "            diff -= 1\n",
    "    assert len(lst) == len(nth_sent_lst)\n",
    "    return nth_sent_lst, ith_start_lst, ith_end_lst\n",
    "    \n",
    "with open(gold_dct_file, \"r\") as infile: \n",
    "    gold_dct = json.load(infile)\n",
    "\n",
    "spacy_better = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "pos_rel = []\n",
    "final_df = pd.DataFrame()\n",
    "cnt = 0\n",
    "# for pat_id in [\"patient47\"]:\n",
    "for pat_id in gold_dct.keys():\n",
    "    note_sub_dir = os.path.join(note_dir, pat_id)\n",
    "    medium_df = pd.DataFrame()\n",
    "    for filename in gold_dct[pat_id].keys():\n",
    "    # for filename in [\"report040_NOTE\"]:\n",
    "\n",
    "        # raw_note = gold_dct[pat_id][filename][\"raw_note\"]\n",
    "        pat_filename = f\"{pat_id}_{filename}.txt\"\n",
    "        raw_note_file = os.path.join(note_sub_dir, pat_filename)\n",
    "        with open(raw_note_file, \"r\") as infile: \n",
    "            lines = infile.readlines()\n",
    "        num_pre9_tokens = len(''.join(lines[:9]))\n",
    "        lines = lines[9:]\n",
    "        clean_note = \"\".join(lines)\n",
    "        doc = spacy_better(clean_note)\n",
    "        # print(f\"len(doc.sents), \", len(list(doc.sents)))\n",
    "\n",
    "        chemo_lst, timex_lst = gold_dct[pat_id][filename][\"pair_wise\"][\"chemo\"][\"ment\"], gold_dct[pat_id][filename][\"pair_wise\"][\"time\"][\"ment\"]\n",
    "        \n",
    "        # print(chemo_lst, timex_lst)\n",
    "        chemo_nth_sent = _find_nth(chemo_lst, \"chemo\")\n",
    "        timex_nth_sent = _find_nth(timex_lst, \"timex\")\n",
    "        \n",
    "        gold_dct[pat_id][filename][\"pair_wise\"][\"chemo\"][\"nth_sent\"] = chemo_nth_sent\n",
    "        gold_dct[pat_id][filename][\"pair_wise\"][\"time\"][\"nth_sent\"] = timex_nth_sent\n",
    "        gold_dct[pat_id][filename][\"num_pre9_tokens\"] = num_pre9_tokens\n",
    "\n",
    "        # create positive instance\n",
    "        gold_pair = gold_dct[pat_id][filename][\"chemo_time_rel\"][\"rel_id\"]\n",
    "        if len(gold_pair) == 0:\n",
    "            continue\n",
    "        gold_sid_pair = gold_dct[pat_id][filename][\"chemo_time_rel\"][\"source_id\"]\n",
    "        gold_tid_pair = gold_dct[pat_id][filename][\"chemo_time_rel\"][\"target_id\"]\n",
    "\n",
    "        num_pre9_tokens = gold_dct[pat_id][filename][\"num_pre9_tokens\"]\n",
    "\n",
    "        rel_type_lst = gold_dct[pat_id][filename][\"chemo_time_rel\"][\"rel_type\"]\n",
    "\n",
    "        chemo_ment_lst, time_ment_lst = gold_dct[pat_id][filename][\"pair_wise\"][\"chemo\"][\"ment\"], gold_dct[pat_id][filename][\"pair_wise\"][\"time\"][\"ment\"]\n",
    "        chemo_span_lst, time_span_lst = gold_dct[pat_id][filename][\"pair_wise\"][\"chemo\"][\"span\"], gold_dct[pat_id][filename][\"pair_wise\"][\"time\"][\"span\"]\n",
    "        chemo_nth_sent_lst, time_nth_sent_lst = gold_dct[pat_id][filename][\"pair_wise\"][\"chemo\"][\"nth_sent\"], gold_dct[pat_id][filename][\"pair_wise\"][\"time\"][\"nth_sent\"]\n",
    "        \n",
    "        chemo_id, time_id = gold_dct[pat_id][filename][\"pair_wise\"][\"chemo\"][\"ment_id\"], gold_dct[pat_id][filename][\"pair_wise\"][\"time\"][\"ment_id\"]\n",
    "        combination = []\n",
    "        for i in range(len(gold_pair)):\n",
    "            pos_ins = defaultdict(dict)\n",
    "            source_id, target_id = gold_sid_pair[i], gold_tid_pair[i]\n",
    "            if source_id in chemo_id: \n",
    "                chemo_ind = chemo_id.index(source_id) \n",
    "                chemo_ment = chemo_ment_lst[chemo_ind]\n",
    "                chemo_start, chemo_end = chemo_span_lst[chemo_ind][0], chemo_span_lst[chemo_ind][1]\n",
    "                chemo_nth_sent = chemo_nth_sent_lst[chemo_ind]\n",
    "                if target_id not in time_id:\n",
    "                    print(pat_id, filename)\n",
    "                time_ind = time_id.index(target_id)\n",
    "                time_ment = time_ment_lst[time_ind]\n",
    "                time_start, time_end = time_span_lst[time_ind][0], time_span_lst[time_ind][1]\n",
    "                time_nth_sent = time_nth_sent_lst[time_ind]\n",
    "            else: \n",
    "                time_ind = time_id.index(source_id) \n",
    "                time_ment = time_ment_lst[time_ind]\n",
    "                time_start, time_end = time_span_lst[time_ind][0], time_span_lst[time_ind][1]\n",
    "                time_nth_sent = time_nth_sent_lst[time_ind]\n",
    "                chemo_ind = chemo_id.index(target_id) \n",
    "                chemo_ment = chemo_ment_lst[chemo_ind]\n",
    "                chemo_start, chemo_end = chemo_span_lst[chemo_ind][0], chemo_span_lst[chemo_ind][1]\n",
    "                chemo_nth_sent = chemo_nth_sent_lst[chemo_ind]\n",
    "\n",
    "            pos_ins[\"filename\"] = f\"{pat_id}_{filename}\"\n",
    "            pos_ins[\"statement\"] = \"Does the arg1 and arg2 has relationship?\"\n",
    "            pos_ins[\"num_pre9_tokens\"] = num_pre9_tokens\n",
    "            pos_ins[\"rel_type\"] = rel_type_lst[i] \n",
    "            pos_ins[\"label\"] = \"positive\"\n",
    "            pos_ins[\"arg1\"] = {\n",
    "                \"mention\":chemo_ment,\n",
    "                \"start_paper_ind\":chemo_start,\n",
    "                \"end_paper_ind\":chemo_end,\n",
    "                \"nth_sent\":chemo_nth_sent\n",
    "            }\n",
    "            pos_ins[\"arg2\"] = {\n",
    "                \"mention\":time_ment,\n",
    "                \"start_paper_ind\":time_start,\n",
    "                \"end_paper_ind\":time_end,\n",
    "                \"nth_sent\":time_nth_sent\n",
    "            }\n",
    "            if int(chemo_nth_sent) > int(time_nth_sent):\n",
    "                start_nth_sent = time_nth_sent\n",
    "                end_nth_sent = chemo_nth_sent\n",
    "            else: \n",
    "                start_nth_sent = chemo_nth_sent\n",
    "                end_nth_sent = time_nth_sent\n",
    "            pos_ins[\"context\"] = _find_sentences_in_range(doc, start_nth_sent, end_nth_sent+1) \n",
    "        \n",
    "            pos_rel.append(pos_ins)\n",
    "\n",
    "\n",
    " \n",
    "        # 2 generate negative sample\n",
    "        chemo_start_ind_tlinkinst, chemo_end_ind_tlinkinst = [], []\n",
    "        nth_sent = []\n",
    "        chemo_lst = []\n",
    "        time_start_ind_tlinkinst, time_end_ind_tlinkinst = [], []\n",
    "        time_nth_sent = []\n",
    "        raw_time_lst = []\n",
    "        match_sents = []\n",
    "\n",
    "        selected_i = []\n",
    "        patient_file = df.loc[df[\"note_name\"] == pat_id + \"_\" + filename]\n",
    "        tsv_df = pd.DataFrame()\n",
    "        selected_row = []\n",
    "        for i, row in patient_file.iterrows():\n",
    "            tlink = row[\"tlink\"]\n",
    "            tlink_inst = row[\"tlink_inst\"]\n",
    "            # if row[\"note_name\"] == \"patient01_report002_NOTE\":\n",
    "            #     print(row[\"tlink_inst\"])\n",
    "            # negtaive sample, rel is none but normed_timex and chemo_text is not NONE\n",
    "            if tlink == \"none\" and row[\"chemo_text\"] != \"none\" and row[\"normed_timex\"] != \"none\":\n",
    "                matches = _find_surrounding_text(tlink_inst)\n",
    "                time_matches = _find_time_surrounding_text(tlink_inst)\n",
    "\n",
    "                selected_row.append(row)\n",
    "                \n",
    "                # if print out, issue with your code\n",
    "                if len(matches) == 0 or len(time_matches) == 0:\n",
    "                    if len(matches) == 0:\n",
    "                        print(\"chemo: yese\", matches)\n",
    "                    elif len(time_matches) == 0:\n",
    "                        print(\"1time: \")\n",
    "\n",
    "                clean_match = matches[0].translate(str.maketrans(string.punctuation, ' ' * len(string.punctuation)))\n",
    "                time_clean_match = time_matches[0].translate(str.maketrans(string.punctuation, ' ' * len(string.punctuation)))\n",
    "                \n",
    "                chemo_lst.append(clean_match)\n",
    "                raw_time_lst.append(time_clean_match) # not exact raw time, but if use the index it is fine\n",
    "                \n",
    "                start_ind, end_ind = find_loc(clean_note, clean_match)\n",
    "                time_start_ind, time_end_ind = _find_time_word_index(clean_note, tlink_inst, time_clean_match)\n",
    "\n",
    "\n",
    "                # if time_start_ind == None and time_end_ind == None:\n",
    "                #     print()\n",
    "                #     time_clean_match = decrease_space_before_punctuation(time_clean_match)\n",
    "                #     time_start_ind, time_end_ind = _find_time_word_index(clean_note, tlink_inst, time_clean_match)\n",
    "             \n",
    "                chemo_start_ind_tlinkinst.append(start_ind)\n",
    "                chemo_end_ind_tlinkinst.append(end_ind)\n",
    "                time_start_ind_tlinkinst.append(time_start_ind)\n",
    "                time_end_ind_tlinkinst.append(time_end_ind)\n",
    "\n",
    "            #     temp_time1 = \"\".join(clean_note[time_start_ind:time_end_ind].split())\n",
    "            #     temp_time2 = \"\".join(time_matches[0].split())\n",
    "            #     temp_time1, temp_time2 = same_characters_in_order(temp_time1, temp_time2)\n",
    "\n",
    "            #     if clean_note[start_ind:end_ind] != matches[0] or temp_time1 !=temp_time2:\n",
    "            #         if clean_note[start_ind:end_ind] != matches[0]:\n",
    "            #             print(\"chemo: no\", pat_id, filename, start_ind, end_ind, matches[0])\n",
    "                        \n",
    "            #         elif clean_note[time_start_ind:time_end_ind] !=time_matches[0]:\n",
    "            #             print(\"2time: \") \n",
    "            #             print(row[\"chemo_annotation_id\"], time_start_ind, time_end_ind, temp_time2, temp_time1, time_clean_match)\n",
    "\n",
    "            # else:\n",
    "                # chemo_start_ind_tlinkinst.append(None)\n",
    "                # chemo_end_ind_tlinkinst.append(None)\n",
    "                # time_start_ind_tlinkinst.append(None)\n",
    "                # time_end_ind_tlinkinst.append(None)\n",
    "                # raw_time_lst.append(None)\n",
    "                continue\n",
    "   \n",
    "        nth_sent_item, ith_start_item, ith_end_item = _df_find_nth(chemo_lst, chemo_start_ind_tlinkinst, chemo_end_ind_tlinkinst) \n",
    "        time_nth_sent_item, time_ith_start_item, time_ith_end_item = _df_find_nth(raw_time_lst, time_start_ind_tlinkinst, time_end_ind_tlinkinst)\n",
    "\n",
    "        tsv_df[\"chemo_tlinkinst_start\"] = chemo_start_ind_tlinkinst\n",
    "        tsv_df[\"chemo_tlinkinst_end\"] = chemo_end_ind_tlinkinst\n",
    "        tsv_df[\"nth_sent\"] = nth_sent_item\n",
    "        # tsv_df[\"ith_start\"] = ith_start_item\n",
    "        # tsv_df[\"ith_end\"] = ith_end_item\n",
    "        tsv_df[\"time_tlinkinst_start\"] = time_start_ind_tlinkinst\n",
    "        tsv_df[\"time_tlinkinst_end\"] = time_end_ind_tlinkinst\n",
    "        tsv_df[\"time_nth_sent\"] = time_nth_sent_item\n",
    "        tsv_df[\"time_ith_start\"] = time_ith_start_item\n",
    "        tsv_df[\"time_ith_end\"] = time_ith_end_item\n",
    "        tsv_df[\"raw_time\"] = raw_time_lst\n",
    "\n",
    "        sent_range_lst = []\n",
    "        for i in range(len(time_nth_sent_item)):\n",
    "            if int(nth_sent_item[i]) > int(time_nth_sent_item[i]):\n",
    "                start_nth_sent = int(time_nth_sent_item[i])\n",
    "                end_nth_sent = int(nth_sent_item[i])\n",
    "            else: \n",
    "                start_nth_sent = int(nth_sent_item[i])\n",
    "                end_nth_sent = int(time_nth_sent_item[i])\n",
    "            sent_range = _find_tsv_sentences_in_range(doc, start_nth_sent, end_nth_sent+1)\n",
    "            # print(\"ent_range: \", sent_range)\n",
    "            sent_range_lst.append(sent_range) \n",
    "        tsv_df[\"match_sents\"] = sent_range_lst\n",
    "        selected_pat_file = patient_file.iloc[selected_i]\n",
    "        selected_pat_file = pd.DataFrame(columns=[\"DCT\", \"patient_id\", \"chemo_text\", \"chemo_annotation_id\", \"normed_timex\", \"timex_annotation_id\", \"tlink\", \"note_name\", \"tlink_inst\"], data=selected_row)\n",
    "        new_df = pd.concat([selected_pat_file.reset_index(), tsv_df.reset_index()], axis=1)\n",
    "        medium_df = pd.concat([new_df, medium_df])\n",
    "    final_df = pd.concat([final_df, medium_df])\n",
    "final_df = final_df.reset_index()\n",
    "\n",
    "final_df.to_csv(f\"./{cancer}_{mode}_neg_rel.tsv\", sep=\"\\t\", index=False)\n",
    "\n",
    "with open(f\"./{cancer}_{mode}_pos_rel.json\", 'w') as outfile:\n",
    "    json.dump(pos_rel, outfile, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['DCT', 'patient_id', 'chemo_text', 'chemo_annotation_id',\n",
       "       'normed_timex', 'timex_annotation_id', 'tlink', 'note_name',\n",
       "       'tlink_inst'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unsummarized_file = f\"/users/the/NER_MTB/timelines/chemoTimelinesBaselineSystem/output/change_n2space/unsummarized_{cancer}_{mode}_output.tsv\"\n",
    "df = pd.read_csv(unsummarized_file, delimiter=\"\\t\")\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "note_dir = os.path.join(\"/users/the/NER_MTB/timelines/chemoTimelinesBaselineSystem/input/change_n2space\", \\\n",
    "                        f\"input_{cancer}_{mode}\", \"Patient_Notes\", f\"{cancer}\", f\"{mode}\")\n",
    "\n",
    "unsummarized_file = f\"/users/the/NER_MTB/timelines/chemoTimelinesBaselineSystem/output/change_n2space/unsummarized_{cancer}_{mode}_output.tsv\"\n",
    "df = pd.read_csv(unsummarized_file, delimiter=\"\\t\")\n",
    "\n",
    "tsv_lst = []\n",
    "pat_filename = np.unique(final_df[\"note_name\"]).tolist()\n",
    "pat_id = pat_filename\n",
    "for uni_pat_filename in pat_filename:\n",
    "    single_dct = defaultdict(dict)\n",
    "    single_df = final_df[final_df[\"note_name\"] == uni_pat_filename]\n",
    "    print(single_df.columns)\n",
    "    for i, row in single_df.iterrows():\n",
    "        print(i, row)\n",
    "        if abs(row[\"nth_sent\"]-row[\"time_nth_sent\"]) <= 1:\n",
    "            single_dct[\"filename\"] = uni_pat_filename\n",
    "            single_dct[\"statement\"] = \"Does the arg1 and arg2 has relationship?\"\n",
    "            single_dct[\"arg1\"] = {\n",
    "                \"mention\": row[\"chemo_text\"],\n",
    "                # \"chemo_tlinkinst_start\": int(row[\"chemo_tlinkinst_start\"]),\n",
    "                # \"chemo_tlinkinst_end\": int(row[\"chemo_tlinkinst_end\"]),\n",
    "                \"nth_sent\": int(row[\"nth_sent\"]),\n",
    "                # \"ith_start\": int(row[\"ith_start\"]),\n",
    "                # \"ith_end\": int(row[\"ith_end\"]),\n",
    "            }\n",
    "            single_dct[\"arg2\"] = {\n",
    "                \"mention\": row[\"raw_time\"],\n",
    "                \"time_tlinkinst_start\": int(row[\"time_tlinkinst_start\"]),\n",
    "                \"time_tlinkinst_end\": int(row[\"time_tlinkinst_end\"]),\n",
    "                \"time_nth_sent\": int(row[\"time_nth_sent\"]),\n",
    "                # \"time_ith_start\": int(row[\"time_ith_start\"]),\n",
    "                # \"time_ith_end\": int(row[\"time_ith_end\"]),\n",
    "            }\n",
    "            \n",
    "            pat_id = uni_pat_filename.split(\"_\")[0]\n",
    "            note_sub_dir = os.path.join(note_dir, pat_id)\n",
    "            pat_filename_txt = uni_pat_filename+\".txt\"\n",
    "            raw_note_file = os.path.join(note_sub_dir, pat_filename_txt)\n",
    "\n",
    "            with open(raw_note_file, \"r\") as infile: \n",
    "                lines = infile.readlines()\n",
    "            num_pre9_tokens = len(''.join(lines[:9]))\n",
    "            single_dct[\"num_pre9_tokens\"] = num_pre9_tokens \n",
    "\n",
    "            lines = lines[9:]\n",
    "            clean_note = \"\".join(lines)\n",
    "            doc = spacy_better(clean_note)\n",
    "            \n",
    "            if int(row[\"nth_sent\"]) < int(row[\"time_nth_sent\"]):\n",
    "                start_sent_ind = int(row[\"nth_sent\"])\n",
    "                end_start_ind = int(row[\"time_nth_sent\"])\n",
    "            else: \n",
    "                start_sent_ind = int(row[\"time_nth_sent\"])\n",
    "                end_start_ind = int(row[\"nth_sent\"])\n",
    "            \n",
    "            temp = []\n",
    "            for i, sent in enumerate(doc.sents):\n",
    "                if i<= end_start_ind and i >= start_sent_ind:\n",
    "                    temp.append(sent.text)\n",
    "            single_dct[\"context\"] = \" \".join(temp)\n",
    "            # temp =\" \".join(temp)\n",
    "            # single_dct[]\n",
    "            tsv_lst.append(single_dct)\n",
    "tsv_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found one pos rel in the tsv file ...\n",
      "found one pos rel in the tsv file ...\n",
      "found one pos rel in the tsv file ...\n",
      "found one pos rel in the tsv file ...\n",
      "found one pos rel in the tsv file ...\n",
      "found one pos rel in the tsv file ...\n",
      "found one pos rel in the tsv file ...\n",
      "found one pos rel in the tsv file ...\n",
      "found one pos rel in the tsv file ...\n",
      "found one pos rel in the tsv file ...\n",
      "found one pos rel in the tsv file ...\n",
      "found one pos rel in the tsv file ...\n"
     ]
    }
   ],
   "source": [
    "neg_lst = []\n",
    "\n",
    "tsv_filename_lst = [x[\"filename\"] for x in tsv_lst]\n",
    "# json exclude pos pair in gold file \n",
    "for pair in pos_rel:\n",
    "    pat_filename = pair[\"filename\"]\n",
    "    if pat_filename in tsv_filename_lst: \n",
    "        tsv_fill_lst = [i for i in tsv_lst if i[\"filename\"]==pat_filename]\n",
    "        for uni in tsv_fill_lst:\n",
    "            # print(uni)\n",
    "            # print(pair)\n",
    "            if (\"\".join(uni[\"arg1\"][\"mention\"]) == \"\".join(pair[\"arg1\"][\"mention\"])) and \\\n",
    "                (\"\".join(uni[\"arg2\"][\"mention\"]) == \"\".join(pair[\"arg2\"][\"mention\"])) and \\\n",
    "                    (uni[\"arg1\"][\"nth_sent\"] == pair[\"arg1\"][\"nth_sent\"]) and \\\n",
    "                         (uni[\"arg2\"][\"time_nth_sent\"] == pair[\"arg2\"][\"nth_sent\"]):\n",
    "                    print(\"found one pos rel in the tsv file ...\")\n",
    "                    continue \n",
    "            else:\n",
    "                uni[\"label\"] = \"negative\"\n",
    "                if uni in neg_lst:\n",
    "                    continue\n",
    "                else:\n",
    "                    neg_lst.append(uni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos_rel[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_loc(a, b):\n",
    "    # a: long one\n",
    "    a = re.sub('[^0-9a-zA-Z]', ' ', a)\n",
    "    b = re.sub('[^0-9a-zA-Z]', ' ', b)\n",
    "    a_clean = ''.join(a.split())\n",
    "    b_clean = ''.join(b.split())\n",
    "    index = a_clean.find(b_clean)\n",
    "    s, out = 0, []\n",
    "    for i, char in enumerate(a):\n",
    "        if char == \" \" or char == \"\\t\":\n",
    "            continue \n",
    "        s+=1 \n",
    "        if s == index + 1: \n",
    "            out.append(i)           \n",
    "        if s == index + len(b_clean):\n",
    "            out.append(i)\n",
    "    if len(out) == 2:\n",
    "        start, end = out[0], out[1]+1\n",
    "        return start, end\n",
    "    else: \n",
    "        return None, None\n",
    "final_pos_lst= []\n",
    "for pos in pos_rel: \n",
    "    context = \" \".join(pos[\"context\"])\n",
    "\n",
    "    ment1 = pos[\"arg1\"][\"mention\"]\n",
    "    ment1\n",
    "    ment2 =pos[\"arg2\"][\"mention\"]\n",
    "    final_pos_lst.append({\n",
    "        \"filename\" :pos[\"filename\"],\n",
    "        \"statement\" :pos[\"statement\"],\n",
    "        \"num_pre9_tokens\":pos[\"num_pre9_tokens\"],\n",
    "        \"rel_type\":pos[\"rel_type\"],\n",
    "        \"label\":pos[\"label\"],\n",
    "        \"arg1\":{\n",
    "            \"mention\":pos[\"arg1\"][\"mention\"],\n",
    "            \"nth_sent\":pos[\"arg1\"][\"nth_sent\"],\n",
    "            \"start\":find_loc(context,ment1)[0],\n",
    "            \"end\":find_loc(context,ment1)[1]\n",
    "        },\n",
    "        \"arg2\":{\n",
    "            \"mention\":pos[\"arg2\"][\"mention\"],\n",
    "            \"nth_sent\":pos[\"arg2\"][\"nth_sent\"],\n",
    "            \"start\":find_loc(context,ment2)[0],\n",
    "            \"end\":find_loc(context,ment2)[1]\n",
    "        },\n",
    "        \"context\":\" \".join(pos[\"context\"])\n",
    "\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pos_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_neg_lst = []\n",
    "\n",
    "for neg in neg_lst:\n",
    "    context = neg[\"context\"]\n",
    "\n",
    "    ment1 = neg[\"arg1\"][\"mention\"]\n",
    "    ment2 =neg[\"arg2\"][\"mention\"]\n",
    "    final_neg_lst.append({\n",
    "        \"filename\" :neg[\"filename\"],\n",
    "        \"statement\" :neg[\"statement\"],\n",
    "        \"num_pre9_tokens\":neg[\"num_pre9_tokens\"],\n",
    "        \"rel_type\":\"none\",\n",
    "        \"label\":\"negative\",\n",
    "        \"arg1\":{\n",
    "            \"mention\":neg[\"arg1\"][\"mention\"],\n",
    "            \"nth_sent\":neg[\"arg1\"][\"nth_sent\"],\n",
    "            \"start\":find_loc(context,ment1)[0],\n",
    "            \"end\":find_loc(context,ment1)[1]\n",
    "        },\n",
    "        \"arg2\":{\n",
    "            \"mention\":neg[\"arg2\"][\"mention\"],\n",
    "            \"nth_sent\":neg[\"arg2\"][\"time_nth_sent\"],\n",
    "            \"start\":find_loc(context,ment2)[0],\n",
    "            \"end\":find_loc(context,ment2)[1]\n",
    "        }, \"context\":context\n",
    "\n",
    "    })\n",
    "\n",
    "    final_neg_lst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113 22\n"
     ]
    }
   ],
   "source": [
    "print(len(final_pos_lst), len(final_neg_lst)) #113, 101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "both_lst = final_pos_lst+final_neg_lst\n",
    "\n",
    "file = f\"/users/the/NER_MTB/timeset/my_code_result/input_data/{cancer}_{mode}_pos_neg_for_nli.json\"\n",
    "with open(file, \"w\") as outfile: \n",
    "    json.dump(both_lst, outfile, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# both_rel = []\n",
    "# for item in pos_rel: \n",
    "#     if len(item[\"arg1\"]) != 0 or len(item[\"mention\"]):\n",
    "#         both_rel.append(item)\n",
    "\n",
    "# for item in both_rel:\n",
    "#     # Extract arg1 and arg2 mention indices from the context\n",
    "#     context = item[\"context\"][0]  # Assuming context is a list with one element\n",
    "#     arg1_start = context.find(item[\"arg1\"][\"mention\"])\n",
    "#     arg1_end = arg1_start + len(item[\"arg1\"][\"mention\"]) if arg1_start != -1 else -1\n",
    "#     arg2_start = context.find(item[\"arg2\"][\"mention\"])\n",
    "#     arg2_end = arg2_start + len(item[\"arg2\"][\"mention\"]) if arg2_start != -1 else -1\n",
    "    \n",
    "#     # Add start_ind and end_ind attributes to the dictionary\n",
    "#     item[\"arg1\"][\"start\"] = arg1_start\n",
    "#     item[\"arg1\"][\"end\"] = arg1_end\n",
    "#     item[\"arg2\"][\"start\"] = arg2_start\n",
    "#     item[\"arg2\"][\"end\"] = arg2_end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged = df.merge(new_df_filtered, on=['DCT', 'patient_id', 'chemo_text', 'chemo_annotation_id',\n",
    "#        'normed_timex', 'timex_annotation_id', 'tlink', 'note_name',\n",
    "#        'tlink_inst'], how=\"outer\", indicator=True)\n",
    "# unique_to_df1 = merged[merged['_merge'] == 'left_only']\n",
    "# unique_to_df2 = merged[merged['_merge'] == 'right_only']\n",
    "# # print(\"Rows unique to df1:\")\n",
    "# # print(unique_to_df1)\n",
    "\n",
    "# print(\"\\nRows unique to df2:\")\n",
    "# print(unique_to_df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. generate candiatate pair from 2, mark the pair as negative if it is not at 1 (need index to compare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# none_count = final_df[\"nth_sent\"].isna().sum()\n",
    "# total_rows = len(final_df)\n",
    "# percentage_none = (none_count / total_rows) * 100 #0.32%\n",
    "# print(f\"Percentage of rows with None in 'nth_sent': {percentage_none:.2f}%\")\n",
    "# final_df.shape #(2182, 24)\n",
    "# same_sent_df = final_df[abs(final_df[\"nth_sent\"].astype(int) - final_df[\"time_nth_sent\"].astype(int)) <= 1]\n",
    "# print(same_sent_df.shape) #(321, 17)\n",
    "# same_sent_df.to_csv(\"./temp_same.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df = final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df = test_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neg_lst[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same_sent_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nnote_dir = os.path.join(\"/users/the/NER_MTB/timelines/chemoTimelinesBaselineSystem/input/change_n2space\",                         f\"input_{cancer}_{mode}\", \"Patient_Notes\", f\"{cancer}\", f\"{mode}\")\\n\\ndef _get_string(row):\\n    return row[0]\\n\\ntf_label_lst = []\\npos_cnt, neg_cnt = 0, 0\\nfor ind, row in same_sent_df[:10].iterrows():\\n    neg_ins = defaultdict(dict)\\n\\n    pat_id = row[\"note_name\"].split(\"_\")[0]\\n    filename = \"_\".join([row[\"note_name\"].split(\"_\")[1], row[\"note_name\"].split(\"_\")[2]])\\n    # print(f\"~~~~{pat_id}, {filename}~~~~\")\\n    pat_filename = f\"{pat_id}_{filename}.txt\"\\n    note_sub_dir = os.path.join(note_dir, pat_id)\\n    raw_note_file = os.path.join(note_sub_dir, pat_filename)\\n    try:\\n        with open(raw_note_file, \"r\") as infile: \\n            lines = infile.readlines()\\n        lines = lines[9:]\\n    except:\\n        print(raw_note_file)\\n        continue\\n    clean_note = \"\".join(lines)\\n    doc = spacy_better(clean_note)\\n    seindexs_single_note, sent_lst = _get_seindexs_single_note(clean_note, spacy_better)\\n    \\n    num_pre9_tokens = int(gold_dct[pat_id][filename][\"num_pre9_tokens\"])\\n    df_chemo_sid, df_chemo_eid = row[\"chemo_tlinkinst_start\"], row[\"chemo_tlinkinst_end\"]\\n    df_time_sid, df_time_eid = row[\"time_tlinkinst_start\"], row[\"time_tlinkinst_end\"]\\n    df_chemo_sid, df_chemo_eid = df_chemo_sid+num_pre9_tokens, df_chemo_eid+num_pre9_tokens\\n    df_time_sid, df_time_eid = int(df_time_sid)+num_pre9_tokens, int(df_time_eid)+num_pre9_tokens\\n    df_nth_sent, df_time_nth_sent = row[\"nth_sent\"], row[\"time_nth_sent\"]\\n    ith_start, ith_end = row[\"ith_start\"], row[\"ith_end\"]\\n    time_ith_start, time_ith_end = row[\"time_ith_start\"], row[\"time_ith_end\"]\\n\\n    chemo_ment, time_ment = row[\"chemo_text\"], row[\"raw_time\"]\\n    if filename not in gold_dct[pat_id].keys(): \\n        neg_cnt +=1\\n        neg_ins[\"filename\"] = row[\"note_name\"]\\n        neg_ins[\"statement\"] = \"Does the arg1 and arg2 has relationship?\"\\n        neg_ins[\"num_pre9_tokens\"] = num_pre9_tokens\\n        neg_ins[\"rel_type\"] = None\\n        neg_ins[\"label\"] = \"negative\"\\n\\n        neg_ins[\"arg1\"] = {\\n            \"mention\":chemo_ment,\\n            \"start_paper_ind\":df_chemo_sid.astype(int),\\n            \"end_paper_ind\":df_chemo_eid.astype(int),\\n            \"nth_sent\":df_nth_sent.astype(int)\\n        }\\n        neg_ins[\"arg2\"] = {\\n            \"mention\":time_ment,\\n            \"start_paper_ind\":df_time_sid.astype(int),\\n            \"end_paper_ind\":df_time_eid.astype(int),\\n            \"nth_sent\":df_time_nth_sent.astype(int)\\n        }\\n        # chemo_nth_sent, time_nth_sent = row[\"nth_sent\"], row[\"time_nth_sent\"]\\n        # if int(chemo_nth_sent) > int(time_nth_sent):\\n        #     start_nth_sent = time_nth_sent\\n        #     end_nth_sent = chemo_nth_sent\\n        # else: \\n        #     start_nth_sent = chemo_nth_sent\\n        #     end_nth_sent = time_nth_sent\\n        row[\"match_sents\"] = row[\"match_sents\"].apply(_get_string)\\n        neg_ins[\"context\"] = row[\"match_sents\"]\\n        both_rel.append(neg_ins)\\n    else: \\n        true_source_id, true_target_id = gold_dct[pat_id][filename][\"chemo_time_rel\"][\"source_id\"], gold_dct[pat_id][filename][\"chemo_time_rel\"][\"target_id\"]\\n        \\n        true_pair = []\\n        for i, j in zip(true_source_id, true_target_id):\\n            true_pair.append(tuple([i, j]))\\n\\n        row_pair = tuple([row[\"timex_annotation_id\"], row[\"chemo_annotation_id\"]])\\n        if any(sorted(row_pair) == sorted(s) for s in true_pair):\\n            print(row_pair, true_pair)\\n            continue\\n    \\n\\n        true_pair_span = []\\n        if pat_id not in gold_dct.keys():\\n            tf_label_lst.append(\"negative\")\\n        else: \\n            if filename not in gold_dct[pat_id].keys():\\n                tf_label_lst.append(\"negative\")\\n            else: \\n                for true_pair in zip(gold_dct[pat_id][filename][\"chemo_time_rel\"][\"source_id\"], gold_dct[pat_id][filename][\"chemo_time_rel\"][\"target_id\"]):\\n                    if true_pair[0] in gold_dct[pat_id][filename][\"pair_wise\"][\"chemo\"][\"ment_id\"]: \\n                        chemo_span = gold_dct[pat_id][filename][\"pair_wise\"][\"chemo\"][\"span\"][gold_dct[pat_id][filename][\"pair_wise\"][\"chemo\"][\"ment_id\"].index(true_pair[0])]\\n                        time_span = gold_dct[pat_id][filename][\"pair_wise\"][\"time\"][\"span\"][gold_dct[pat_id][filename][\"pair_wise\"][\"time\"][\"ment_id\"].index(true_pair[1])]\\n                    else: \\n                        chemo_span = gold_dct[pat_id][filename][\"pair_wise\"][\"chemo\"][\"span\"][gold_dct[pat_id][filename][\"pair_wise\"][\"chemo\"][\"ment_id\"].index(true_pair[1])]\\n                        time_span = gold_dct[pat_id][filename][\"pair_wise\"][\"time\"][\"span\"][gold_dct[pat_id][filename][\"pair_wise\"][\"time\"][\"ment_id\"].index(true_pair[0])]\\n                    # print(chemo_span, time_span)\\n                    true_pair_span.append((chemo_span, time_span))\\n                if ([df_chemo_sid, df_chemo_eid],[df_time_sid, df_time_eid]) in true_pair_span or                 ([df_time_sid, df_time_eid], [df_chemo_sid, df_chemo_eid]) in true_pair_span:\\n                    pos_cnt += 1\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "note_dir = os.path.join(\"/users/the/NER_MTB/timelines/chemoTimelinesBaselineSystem/input/change_n2space\", \\\n",
    "                        f\"input_{cancer}_{mode}\", \"Patient_Notes\", f\"{cancer}\", f\"{mode}\")\n",
    "\n",
    "def _get_string(row):\n",
    "    return row[0]\n",
    "\n",
    "tf_label_lst = []\n",
    "pos_cnt, neg_cnt = 0, 0\n",
    "for ind, row in same_sent_df[:10].iterrows():\n",
    "    neg_ins = defaultdict(dict)\n",
    "\n",
    "    pat_id = row[\"note_name\"].split(\"_\")[0]\n",
    "    filename = \"_\".join([row[\"note_name\"].split(\"_\")[1], row[\"note_name\"].split(\"_\")[2]])\n",
    "    # print(f\"~~~~{pat_id}, {filename}~~~~\")\n",
    "    pat_filename = f\"{pat_id}_{filename}.txt\"\n",
    "    note_sub_dir = os.path.join(note_dir, pat_id)\n",
    "    raw_note_file = os.path.join(note_sub_dir, pat_filename)\n",
    "    try:\n",
    "        with open(raw_note_file, \"r\") as infile: \n",
    "            lines = infile.readlines()\n",
    "        lines = lines[9:]\n",
    "    except:\n",
    "        print(raw_note_file)\n",
    "        continue\n",
    "    clean_note = \"\".join(lines)\n",
    "    doc = spacy_better(clean_note)\n",
    "    seindexs_single_note, sent_lst = _get_seindexs_single_note(clean_note, spacy_better)\n",
    "    \n",
    "    num_pre9_tokens = int(gold_dct[pat_id][filename][\"num_pre9_tokens\"])\n",
    "    df_chemo_sid, df_chemo_eid = row[\"chemo_tlinkinst_start\"], row[\"chemo_tlinkinst_end\"]\n",
    "    df_time_sid, df_time_eid = row[\"time_tlinkinst_start\"], row[\"time_tlinkinst_end\"]\n",
    "    df_chemo_sid, df_chemo_eid = df_chemo_sid+num_pre9_tokens, df_chemo_eid+num_pre9_tokens\n",
    "    df_time_sid, df_time_eid = int(df_time_sid)+num_pre9_tokens, int(df_time_eid)+num_pre9_tokens\n",
    "    df_nth_sent, df_time_nth_sent = row[\"nth_sent\"], row[\"time_nth_sent\"]\n",
    "    ith_start, ith_end = row[\"ith_start\"], row[\"ith_end\"]\n",
    "    time_ith_start, time_ith_end = row[\"time_ith_start\"], row[\"time_ith_end\"]\n",
    "\n",
    "    chemo_ment, time_ment = row[\"chemo_text\"], row[\"raw_time\"]\n",
    "    if filename not in gold_dct[pat_id].keys(): \n",
    "        neg_cnt +=1\n",
    "        neg_ins[\"filename\"] = row[\"note_name\"]\n",
    "        neg_ins[\"statement\"] = \"Does the arg1 and arg2 has relationship?\"\n",
    "        neg_ins[\"num_pre9_tokens\"] = num_pre9_tokens\n",
    "        neg_ins[\"rel_type\"] = None\n",
    "        neg_ins[\"label\"] = \"negative\"\n",
    "\n",
    "        neg_ins[\"arg1\"] = {\n",
    "            \"mention\":chemo_ment,\n",
    "            \"start_paper_ind\":df_chemo_sid.astype(int),\n",
    "            \"end_paper_ind\":df_chemo_eid.astype(int),\n",
    "            \"nth_sent\":df_nth_sent.astype(int)\n",
    "        }\n",
    "        neg_ins[\"arg2\"] = {\n",
    "            \"mention\":time_ment,\n",
    "            \"start_paper_ind\":df_time_sid.astype(int),\n",
    "            \"end_paper_ind\":df_time_eid.astype(int),\n",
    "            \"nth_sent\":df_time_nth_sent.astype(int)\n",
    "        }\n",
    "        # chemo_nth_sent, time_nth_sent = row[\"nth_sent\"], row[\"time_nth_sent\"]\n",
    "        # if int(chemo_nth_sent) > int(time_nth_sent):\n",
    "        #     start_nth_sent = time_nth_sent\n",
    "        #     end_nth_sent = chemo_nth_sent\n",
    "        # else: \n",
    "        #     start_nth_sent = chemo_nth_sent\n",
    "        #     end_nth_sent = time_nth_sent\n",
    "        row[\"match_sents\"] = row[\"match_sents\"].apply(_get_string)\n",
    "        neg_ins[\"context\"] = row[\"match_sents\"]\n",
    "        both_rel.append(neg_ins)\n",
    "    else: \n",
    "        true_source_id, true_target_id = gold_dct[pat_id][filename][\"chemo_time_rel\"][\"source_id\"], gold_dct[pat_id][filename][\"chemo_time_rel\"][\"target_id\"]\n",
    "        \n",
    "        true_pair = []\n",
    "        for i, j in zip(true_source_id, true_target_id):\n",
    "            true_pair.append(tuple([i, j]))\n",
    "\n",
    "        row_pair = tuple([row[\"timex_annotation_id\"], row[\"chemo_annotation_id\"]])\n",
    "        if any(sorted(row_pair) == sorted(s) for s in true_pair):\n",
    "            print(row_pair, true_pair)\n",
    "            continue\n",
    "    \n",
    "\n",
    "        true_pair_span = []\n",
    "        if pat_id not in gold_dct.keys():\n",
    "            tf_label_lst.append(\"negative\")\n",
    "        else: \n",
    "            if filename not in gold_dct[pat_id].keys():\n",
    "                tf_label_lst.append(\"negative\")\n",
    "            else: \n",
    "                for true_pair in zip(gold_dct[pat_id][filename][\"chemo_time_rel\"][\"source_id\"], gold_dct[pat_id][filename][\"chemo_time_rel\"][\"target_id\"]):\n",
    "                    if true_pair[0] in gold_dct[pat_id][filename][\"pair_wise\"][\"chemo\"][\"ment_id\"]: \n",
    "                        chemo_span = gold_dct[pat_id][filename][\"pair_wise\"][\"chemo\"][\"span\"][gold_dct[pat_id][filename][\"pair_wise\"][\"chemo\"][\"ment_id\"].index(true_pair[0])]\n",
    "                        time_span = gold_dct[pat_id][filename][\"pair_wise\"][\"time\"][\"span\"][gold_dct[pat_id][filename][\"pair_wise\"][\"time\"][\"ment_id\"].index(true_pair[1])]\n",
    "                    else: \n",
    "                        chemo_span = gold_dct[pat_id][filename][\"pair_wise\"][\"chemo\"][\"span\"][gold_dct[pat_id][filename][\"pair_wise\"][\"chemo\"][\"ment_id\"].index(true_pair[1])]\n",
    "                        time_span = gold_dct[pat_id][filename][\"pair_wise\"][\"time\"][\"span\"][gold_dct[pat_id][filename][\"pair_wise\"][\"time\"][\"ment_id\"].index(true_pair[0])]\n",
    "                    # print(chemo_span, time_span)\n",
    "                    true_pair_span.append((chemo_span, time_span))\n",
    "                if ([df_chemo_sid, df_chemo_eid],[df_time_sid, df_time_eid]) in true_pair_span or \\\n",
    "                ([df_time_sid, df_time_eid], [df_chemo_sid, df_chemo_eid]) in true_pair_span:\n",
    "                    pos_cnt += 1\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = ('1@e@patient47_report034_NOTE@system', '2@e@patient47_report034_NOTE@system')\n",
    "# b = [('2@e@patient47_report034_NOTE@system', '1@e@patient47_report034_NOTE@system'), ('1@e@patient30_report006_RAD@system', '5@e@patient30_report006_RAD@system')]\n",
    "\n",
    "# if any(sorted(a) == sorted(s) for s in b):\n",
    "#     print(\"yes\")\n",
    "# else:\n",
    "#     print(\"no\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "                # else: \n",
    "                    # neg_ins[\"filename\"] = row[\"note_name\"]\n",
    "                    # neg_ins[\"statement\"] = \"Does the arg1 and arg2 has relationship?\"\n",
    "                    # neg_ins[\"num_pre9_tokens\"] = num_pre9_tokens\n",
    "                    # neg_ins[\"rel_type\"] = None\n",
    "                    # neg_ins[\"label\"] = \"negative\"\n",
    "\n",
    "                    # neg_ins[\"arg1\"] = {\n",
    "                    #     \"mention\":chemo_ment,\n",
    "                    #     \"start_paper_ind\":df_chemo_sid,\n",
    "                    #     \"end_paper_ind\":df_chemo_eid,\n",
    "                    #     \"nth_sent\":df_nth_sent,\n",
    "                    #     \"start\":ith_start,\n",
    "                    #     \"end\": ith_end\n",
    "                    # }\n",
    "                    # neg_ins[\"arg2\"] = {\n",
    "                    #     \"mention\":time_ment,\n",
    "                    #     \"start_paper_ind\":df_time_sid,\n",
    "                    #     \"end_paper_ind\":df_time_eid,\n",
    "                    #     \"nth_sent\":df_time_nth_sent,\n",
    "                    #     \"start\":time_ith_start,\n",
    "                    #     \"end\":time_ith_end\n",
    "                    # }\n",
    "                    # print(type(df_chemo_sid), df_chemo_sid)\n",
    "                    # neg_ins[\"filename\"] = row[\"note_name\"]\n",
    "                    # neg_ins[\"statement\"] = \"Does the arg1 and arg2 has relationship?\"\n",
    "                    # neg_ins[\"num_pre9_tokens\"] = num_pre9_tokens\n",
    "                    # neg_ins[\"rel_type\"] = None\n",
    "                    # neg_ins[\"label\"] = \"negative\"\n",
    "\n",
    "                    # neg_ins[\"arg1\"] = {\n",
    "                    #     \"mention\":chemo_ment,\n",
    "                    #     \"start_paper_ind\":df_chemo_sid.astype(int),\n",
    "                    #     \"end_paper_ind\":df_chemo_eid.astype(int),\n",
    "                    #     \"nth_sent\":df_nth_sent.astype(int)\n",
    "                    # }\n",
    "                    # neg_ins[\"arg2\"] = {\n",
    "                    #     \"mention\":time_ment,\n",
    "                    #     \"start_paper_ind\":df_time_sid.astype(int),\n",
    "                    #     \"end_paper_ind\":df_time_eid.astype(int),\n",
    "                    #     \"nth_sent\":df_time_nth_sent.astype(int)\n",
    "                    # }\n",
    "\n",
    "                    # row[\"match_sents\"] = row[\"match_sents\"].apply(_get_string)\n",
    "                    # neg_ins[\"context\"] = row[\"match_sents\"]\n",
    "                    # both_rel.append(neg_ins)\n",
    "\n",
    "# for item in both_rel:\n",
    "#     if item[\"label\"] == \"negative\":\n",
    "#         # Extract arg1 and arg2 mention indices from the context\n",
    "#         context = item[\"context\"]  # Assuming context is a list with one element\n",
    "#         arg1_start = context.index(item[\"arg1\"][\"mention\"])\n",
    "#         arg1_end = arg1_start + len(item[\"arg1\"][\"mention\"]) #if arg1_start != -1 else -1\n",
    "#         arg2_start = context.index(item[\"arg2\"][\"mention\"])\n",
    "#         arg2_end = arg2_start + len(item[\"arg2\"][\"mention\"]) #if arg2_start != -1 else -1\n",
    "        \n",
    "#         # Add start_ind and end_ind attributes to the dictionary\n",
    "#         item[\"arg1\"][\"start\"] = arg1_start\n",
    "#         item[\"arg1\"][\"end\"] = arg1_end\n",
    "#         item[\"arg2\"][\"start\"] = arg2_start\n",
    "#         item[\"arg2\"][\"end\"] = arg2_end\n",
    "\n",
    "# with open(f\"./{cancer}_{mode}_input.json\", 'w') as outfile:\n",
    "#     json.dump(both_rel, outfile, indent=4)\n",
    "\n",
    "# print(neg_cnt, pos_cnt) \n",
    "# two sent range: 595 225"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from spacy.tokens import Span\n",
    "# for index, item in enumerate(pos_rel):\n",
    "#     if isinstance(item, Span):\n",
    "#         print(f\"Span object found at index {index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnt = 0\n",
    "# for item in pos_rel: \n",
    "#     if len(item[\"arg1\"]) == 0:\n",
    "#         cnt+=1\n",
    "# cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(pos_rel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# both_rel = []\n",
    "# for item in pos_rel: \n",
    "#     if len(item[\"arg1\"]) != 0:\n",
    "#         both_rel.append(item)\n",
    "# len(both_rel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for item in both_rel: \n",
    "#     if len(item[\"context\"]) == 0:\n",
    "#         print(item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_rel = []\n",
    "# for item in both_rel: \n",
    "#     if len(item[\"context\"]) != 0:\n",
    "#         final_rel.append(item)\n",
    "# len(final_rel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Assuming data_list is your list of dictionaries\n",
    "# for item in final_rel:\n",
    "#     # Extract arg1 and arg2 mention indices from the context\n",
    "#     context = item[\"context\"][0]  # Assuming context is a list with one element\n",
    "#     arg1_start = context.find(item[\"arg1\"][\"mention\"])\n",
    "#     arg1_end = arg1_start + len(item[\"arg1\"][\"mention\"]) if arg1_start != -1 else -1\n",
    "#     arg2_start = context.find(item[\"arg2\"][\"mention\"])\n",
    "#     arg2_end = arg2_start + len(item[\"arg2\"][\"mention\"]) if arg2_start != -1 else -1\n",
    "    \n",
    "#     # Add start_ind and end_ind attributes to the dictionary\n",
    "#     item[\"arg1\"][\"start\"] = arg1_start\n",
    "#     item[\"arg1\"][\"end\"] = arg1_end\n",
    "#     item[\"arg2\"][\"start\"] = arg2_start\n",
    "#     item[\"arg2\"][\"end\"] = arg2_end\n",
    "\n",
    "# with open(f'./{cancer}_{mode}.json', 'w') as f:\n",
    "#     json.dump(final_rel, f, indent=4) \n",
    "# len(final_rel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# both_rel"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
